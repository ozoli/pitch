choose(4,1)*choose(13,5)
5108/2598960
choose(1,13)*choose(4,4)*choose(1,12)*choose(1,4)/choose(5,52)
13*12*4/2598960
0.1055
skip
question
1/52+13/51+12/50+11/49+1/48
1/52*13/51*12/50*11/49*1/48
1/52+13/51+12/50+11/49+1/48
1/52*13/51*12/50*11/49*1/48
0.168
0.000240
0.00024010
0.00024010
0.001087
0.024010
info()
skip()
16/52
12/52
2/51
swirl()
1/52*13/51*12/50*11/49*1/48
library(swirl)
swirl()
install_from_swirl("Regression Models")
swirl()
plot(child ~ parent, galton)
plot(jitter(child,4) ~ parent, galton)
regrline <- lm(child ~ parent, galton)
abline(regrline, lwd=3, col='red')
summary(regrline)
lm(child ~ parent, galton)
fit <- lm(child ~ parent, galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals, galton$parent)
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
lhs - rhx
lhs - rhs
all.equal(lhs, rhs)
varChild <- var(rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
varEst <- var(rhs)
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild, varRes + varEst)
efit <- lm(accel ~ mag+dist, attenu)
mean(efit$residuals)
cov(efit$residuals, attenu$mag)
cov(efit$residuals, attenu$dist)
library('XML')
u="http://www.timeanddate.com/calendar/custom.html?year=2015&country=5&typ=0&display=2&cols=0&hol=0&cdt=1&holm=1&df=1"
tables = readHTMLTable(u)
tables[1]
tables = tables[-1]
tables = tables[-1]
tables = tables[-13]
names(tables) <- paste('month', 1:12, sep = '')
mtables = mapply(cbind, tables, 'Month'= 1:12, SIMPLIFY=F)
do.call('rbind', mtables)
mtables = mapply(cbind, tables, 'Month'=1:12, SIMPLIFY=F)
do.call('rbind', mtables)
?rbind
rbind(mtables)
mtables = mapply(cbind, tables, 'Month'=1:12, SIMPLIFY=F)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
cor(x,w)
cor(w,x)
lsfit(x,w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lsfit(x,y)
data(mtcars)
m <- data(mtcard)
m <- data(mtcars)
m
require(graphics)
pairs(mtcars, main = "mtcars data")
coplot(mpg ~ disp | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1)
mpg
summary(mpg)
fit(mpg ~ disp)
fit <- lm(mpg ~ disp)
fit <- lm(mtcars[mpg] ~ disp)
fit <- lm(mtcars$mpg ~ mtcars$wt)
fit
fit <- lm(mtcars$wt ~ mtcars$mpg)
fit
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
rnorm(x)
norm(x)
colMeans(x)
normalized = (x-min(x))/(max(x)-min(x))
normalized
normalize(x)
library(som)
install.packages('som')
normalize(x)
library(som)
normalize(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit <- lm(x ~ y)
fit
fit <- lm(y ~ x)
fit
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
fit <- lm(x ~ x)
fit <- lm(x ~ x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
sum( (x - mean(x) )^2 )
sum( (x - mean(x) )^2 )
/var(x)
sum( (x - mean(x) )^2 )/var(x)
cor(x,x)
0.51/0.5
data(mtcars)
summary(mtcars)
info(mtcars)
head(mtcars)
str(mtcars)
summary(mtcars)
head(mtcars)
summary(mtcars)
head(mtcars)
require(graphics)
pairs(mtcars, main = "mtcars data")
coplot(mpg ~ disp | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1)
data(mtcars)
lm(formula = mpg ~ am + wt + hp + disp + cyl, data = mtcars)
test_vars <- c('mpg','cyl','disp','hp','wt','gear','carb')
pairs(x = mtcars[,test_vars],
panel = panel.smooth,
main = 'MTCars Attributes')
test_vars <- c('mpg','cyl','disp','hp','wt','gear','carb', 'am')
pairs(x = mtcars[,test_vars],
panel = panel.smooth,
main = 'MTCars Attributes')
attr <- c('mpg','cyl','disp','hp','drat','wt','qsec','vs','am','gear','carb')
pairs(x = mtcars[,attr], panel = panel.smooth, main = 'MTCars Attributes')
+        panel = panel.smooth, rows = 1)
coplot(mpg ~ am, data = mtcars, panel = panel.smooth, rows = 1)
coplot(mpg ~ am, data = mtcars, panel = panel.smooth, rows = 1)
coplot(mpg ~ disp | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1)
coplot(mpg ~ am | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1)
coplot(mpg ~ am | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1, title = 'Number of Cyclinders')
?coplot
coplot(mpg ~ am | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1)
coplot(mpg ~ am | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1, xlab='Transmission Type', ylab="Miles per Gallon (MPG)")
?coplot
coplot(mpg ~ am | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1, xlab='Transmission Type', ylab="Miles per Gallon (MPG)")
coplot(mpg ~ am | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1, xlab=c('Transmission Type', 'top title'), ylab="Miles per Gallon (MPG)")
coplot(mpg ~ am | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1, xlab=c('Transmission Type', 'Number of Cyclinders'), ylab="Miles per Gallon (MPG)")
coplot(mpg ~ am | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1, xlab=c('Transmission Type', 'Number of Cyclinders'), ylab="Miles per Gallon (MPG)")
clear
fit1 <- lm(mpg ~ factor(am), data=mtcars); summary(fit1)$coef
fit1 <- lm(mpg ~ factor(am), data=mtcars); summary(fit1)$coef
summary(fit1)
summary(fit1)$r-squared
summary(fit1)$rsquared
summary(fit1)$var
summary(fit)$r.squared
summary(fit1)$r.squared
fit1 <- lm(mpg ~ factor(am), data=mtcars); summary(fit1)$coef; summary(fit1)$r.squared
names(fit1)
names(summary(fit1))
summary(fit1)$adj.r.squared
fit <- lm(mpg ~ . ,data=mtcars); sqrt(vif(fit));cor(mtcars)[1,]
library(car)
install.packages('car')
library(car)
fit <- lm(mpg ~ . ,data=mtcars); sqrt(vif(fit));cor(mtcars)[1,]
fit <- lm(mpg ~ . ,data=mtcars)
sqrt(vif(fit))
cor(mtcars)[1,]
?vif
fit2 <- lm(mpg ~ am+wt+hp+disp+cyl, data = mtcars); anova(fit1, fit2)
summary(fit2)$p.value
summary(fit2)$adj.r.squared
names(summary(fit2))
names(fit2)
plot(density(resid(fit2)))
layout(matrix(c(1,2,3,4),2,2))
plot(fit2)
names(summary(fit2))
summary(fit2)$r.squared
fit2$coefficients
We see above the multivariate model is significantly different from the simple model becuase the p-value of 4.5e-08 is so very small. The residual plots in Figure 3 of the Appendix show a normally distributed and homoskedastic multivariate model. The R-squared value of 85.5% above shows the amount of variance included in this model. We see wt and cyl confound the model, however there is a 1.55 MPG for manual transmission and therefore we can conclude yes to the question; Is an automatic or manual transmission better for MPG?
We see above the multivariate model is significantly different from the simple model becuase the p-value of 4.5e-08 is so very small. The residual plots in Figure 3 of the Appendix show a normally distributed and homoskedastic multivariate model. The R-squared value of 85.5% above shows the amount of variance included in this model. We see wt and cyl confound the model, however there is a 1.55 MPG for manual transmission and therefore we can conclude yes to the question; Is an automatic or manual transmission better for MPG?
?pairs
install.packages("pairsD3")
require("pairsD3")
pairsD3(x = mtcars[,attr], panel = panel.smooth, main = 'Moter Trends Car Data Attributes')
attr <- c('mpg','cyl','disp','hp','drat','wt','am','gear','carb')
pairs(x = mtcars[,attr], panel = panel.smooth, main = 'Moter Trends Car Data Attributes')
attr <- c('mpg','cyl','disp','hp','wt','am','gear','carb')
pairs(x = mtcars[,attr], panel = panel.smooth, main = 'Moter Trends Car Data Attributes')
fit3 <- lm(mpg ~ wt+cyl, data = mtcars);
summary(fit3)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
summary(fit)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl), data = mtcars)
fit2$coefficients[3]
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl), data = mtcars)
fit1$coefficients[3]
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
compare <- anova(fit1, fit2)
compare$Pr
fit5 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
summary(fit5)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl), data = mtcars)
fit5$coefficients
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
hatvalues(fit)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
hatvalues(fit)
dfbetas(fit)
x <- seq(-10, 10, length = 1000)
manipulate(
plot(x, exp(beta0 + beta1 * x) / (1 + exp(beta0 + beta1 * x)),
type = "l", lwd = 3, frame = FALSE),
beta1 = slider(-2, 2, step = .1, initial = 2),
beta0 = slider(-2, 2, step = .1, initial = 0)
)
install.packages('manipulate')
library(manipulate)
x <- seq(-10, 10, length = 1000)
manipulate(
plot(x, exp(beta0 + beta1 * x) / (1 + exp(beta0 + beta1 * x)),
type = "l", lwd = 3, frame = FALSE),
beta1 = slider(-2, 2, step = .1, initial = 2),
beta0 = slider(-2, 2, step = .1, initial = 0)
)
logRegRavens <- glm(ravensData$ravenWinNum ~ ravensData$ravenScore,family="binomial")
summary(logRegRavens)
download.file("https://dl.dropboxusercontent.com/u/7710864/data/ravensData.rda"
, destfile="./data/ravensData.rda",method="curl")
load("./data/ravensData.rda")
head(ravensData)
download.file("https://dl.dropboxusercontent.com/u/7710864/data/ravensData.rda"
, destfile="./data/ravensData.rda",method="curl")
ravensData <- load("./data/ravensData.rda")
head(ravensData)
download.file("https://dl.dropboxusercontent.com/u/7710864/data/ravensData.rda"
, destfile="./data/ravensData.rda",method="curl")
ravensData <- load("./data/ravensData.rda")
head(ravensData)
ravensData <- load("./data/ravensData.rda")
download.file("https://dl.dropboxusercontent.com/u/7710864/data/ravensData.rda"
, destfile="./ravensData.rda",method="curl")
ravensData <- load("./ravensData.rda")
head(ravensData)
logRegRavens <- glm(ravensData$ravenWinNum ~ ravensData$ravenScore,family="binomial")
summary(logRegRavens)
download.file("https://dl.dropboxusercontent.com/u/7710864/data/ravensData.rda"
, destfile="./ravensData.rda",method="curl")
load("./ravensData.rda")
head(ravensData)
logRegRavens <- glm(ravensData$ravenWinNum ~ ravensData$ravenScore,family="binomial")
summary(logRegRavens)
plot(ravensData$ravenScore,logRegRavens$fitted,pch=19,col="blue",xlab="Score",ylab="Prob Ravens Win")
par(mfrow = c(1, 3))
plot(0 : 10, dpois(0 : 10, lambda = 2), type = "h", frame = FALSE)
plot(0 : 20, dpois(0 : 20, lambda = 10), type = "h", frame = FALSE)
plot(0 : 200, dpois(0 : 200, lambda = 100), type = "h", frame = FALSE)
library(MASS)
data(shuttle)
# convert outcome to 0 = noauto, 1 = auto
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
fit1 <- glm(use ~ wind - 1, data = shuttle, family = "binomial")
summary(fit)
windhead <- fit1$coef[1]
windtail <- fit1$coef[2]
exp(windtail)/exp(windhead)
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
# Question 2
# Consider the previous problem. Give the estimated odds ratio for autoloader
# use comparing head winds (numerator) to tail winds (denominator) adjusting for
# wind strength from the variable magn.
fit2 <- glm(use ~ wind + magn - 1, data = shuttle, family = "binomial")
summary(fit)
windhead2 <- fit2$coef[1]
windtail2 <- fit2$coef[2]
exp(windtail2)/exp(windhead2)
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
# Question 2
# Consider the previous problem. Give the estimated odds ratio for autoloader
# use comparing head winds (numerator) to tail winds (denominator) adjusting for
# wind strength from the variable magn.
fit2 <- glm(use ~ wind + magn - 1, data = shuttle, family = "binomial")
summary(fit)
windhead2 <- fit2$coef[1]
windtail2 <- fit2$coef[2]
exp(windtail2)/exp(windhead2)
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
fit2 <- glm(use ~ wind + magn - 1, data = shuttle, family = "binomial")
library(MASS)
data(shuttle)
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
fit2 <- glm(use ~ wind + magn - 1, data = shuttle, family = "binomial")
summary(fit)
windhead2 <- fit2$coef[1]
windtail2 <- fit2$coef[2]
exp(windtail2)/exp(windhead2)
library(MASS)
data(shuttle)
shuttle$auto <- as.numeric(shuttle$use=="auto")
fit <- glm(auto ~ wind,  binomial,  shuttle)
fit3 <- glm(1-auto ~ wind,  binomial, shuttle)
fit$coefficients
fit3$coefficients
data(InsectSprays)
fit <- glm(count ~ spray  - 1, family = "poisson", data = InsectSprays)
exp(fit$coef[1])/exp(fit$coef[2])
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
glm(count ~ x + offset(t), family = poisson)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
lhs <- function(x) ifelse(x < 0,0-x,0) # basis function 1 (lhs = left hockey stick)
rhs <- function(x) ifelse(x > 0,x-0,0) # basis function 2 (rhs = right hockey stick)
gb <- lm(y ~ lhs(x) + rhs(x))
x <- seq(-5,5,by=1)
py <- gb$coef[1]+gb$coef[2]*lhs(x)+gb$coef[3]*rhs(x)
lines(x, py)
lhs <- function(x) ifelse(x < 0,0-x,0)
rhs <- function(x) ifelse(x > 0,x-0,0)
gb <- lm(y ~ lhs(x) + rhs(x))
x <- seq(-5,5,by=1)
py <- gb$coef[1]+gb$coef[2]*lhs(x)+gb$coef[3]*rhs(x)
lines(x, py)
py
?lines
plot(x,y)
lines(x, py)
mtcars$info
?mtcars
TP <- 99
FN <- 1
TN <- 0.99 * 99900
FP <- 99900 - TN
PPV <- TP / (TP + FP)
PPV
install.packages('AppliedPredictiveModeling')
install.packages('carret')
install.packages('caret')
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
training$index <- seq(1, nrow(training))
require(reshape2)
D <- melt(training, id.var=c("index"))
ggplot(D, aes(x=index, y=value, color=variable)) +
geom_point(alpha=1/2) +
geom_smooth(alpha=1/2) +
facet_wrap(~ variable, nrow=3, scales="free_y") +
theme(legend.position="none")
ggplot(training, aes(x=Cement, y=CompressiveStrength)) +
geom_point(alpha=1/2) +
geom_smooth(alpha=1/2) +
geom_rug(alpha=1/4)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
qplot(Superplasticizer, data=training, geom="histogram")
table(training$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
predVar <- grep("^IL", names(training))
str(training[predVar])
preProcess(training[predVar], method="pca", thresh=0.8)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
predVar <- grep("^IL", names(training))
str(training[predVar])
preProcess(training[predVar], method="pca", thresh=0.9)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
predVar <- grep("^IL", names(training))
M0 <- train(training$diagnosis ~ ., data=training[predVar], method="glm")
hat0 <- predict(M0, testing)
confusionMatrix(testing$diagnosis, hat0)
preProc <- preProcess(training[predVar], method="pca", thresh=0.8)
trainPC <- predict(preProc, training[predVar])
M1 <- train(training$diagnosis ~ ., data=trainPC, method="glm")
testPC <- predict(preProc, testing[predVar])
hat1 <- predict(M1, testPC)
confusionMatrix(testing$diagnosis, hat1)
install.packages('e1071')
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
predVar <- grep("^IL", names(training))
M0 <- train(training$diagnosis ~ ., data=training[predVar], method="glm")
hat0 <- predict(M0, testing)
confusionMatrix(testing$diagnosis, hat0)
preProc <- preProcess(training[predVar], method="pca", thresh=0.8)
trainPC <- predict(preProc, training[predVar])
M1 <- train(training$diagnosis ~ ., data=trainPC, method="glm")
testPC <- predict(preProc, testing[predVar])
hat1 <- predict(M1, testPC)
confusionMatrix(testing$diagnosis, hat1)
require(utils)
setInternet2(TRUE)
?setInternet
??setInternet
??setInternet2
install.packages('doParallel')
library(randomForest)
install.packages(randomForest')
install.packages('randomForest')
install.packages('randomForest')
url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainingData <- fread(url)
require(data.table)
url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainingData <- fread(url)
isAnyMissing <- sapply(trainingData, function (x) any(is.na(x) | x == ""))
isPredictor <- !isAnyMissing & grepl("belt|[^(fore)]arm|dumbbell|forearm", names(isAnyMissing))
predCandidates <- names(isAnyMissing)[isPredictor]
predCandidates
setwd("~/DataScience/DevelopingDataProducts")
library("slidify")
slidify("pitch.Rmd")
slidify("pitch.Rmd")
slidify("pitch.Rmd")
slidify("pitch.Rmd")
slidify("pitch.Rmd")
slidify("pitch.Rmd")
slidify("pitch.Rmd")
slidify("pitch.Rmd")
slidify("pitch.Rmd")
setwd("~/DataScience/DevelopingDataProducts/pitch")
slidify("pitch.Rmd")
slidify("pitch.Rmd")
